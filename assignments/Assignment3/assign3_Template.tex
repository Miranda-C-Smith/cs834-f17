\documentclass[letterpaper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{verbatim}
\usepackage[parfill]{parskip}


\title{Information Retrieval: Assignment 3}
\author{Sawood Alam\\ salam@cs.odu.edu}
\begin{document}
\maketitle
\date{}

\begin{abstract}
This document contains solutions and discussions on exercise questions 6.1, 6.3, 6.4, 6.5, 6.7, 7.2, 7.5 and 7.8 from the textbook, Search Engines Information Retrieval in Practice - W. Bruce Croft, Donald Metzler and Trevor Strohman.
\end{abstract}
\pagebreak

\section{Problem 6.1}

Using the Wikipedia collection provided at the book website, create a sample of stem clusters by the the following process.

\begin{itemize}
\item Index the collection without stemming.
\item Identify the first 1,000 words (in alphabetical order) in the index.
\item Create stem class by stemming these 1,000 words and recording which words become the same stem.
\item Compute association measures (Dice's coefficient)between all pairs of stems in each stem class. Compute co-occurrence at the document level.
\item Create stem clusters by thresholding the association measure. All terms that are still connected to each other from the cluster.
\end{itemize}

Compare the stem clusters to the stem classes in terms of size and quality (in your opinion) of the groupings.


\subsection{Solution}

To solve this question, I used Galago, custom Ruby codes and some manual processing.

\subsubsection{Indexing}

I downloaded and ran the Galago binary file to index my small Wikipedia collection provided on book's website (Figure \ref{gi}). I was thinking of using my custom code (that I wrote in last assignment) for indexing. But, I thought of trying Galago this time. It took around 45 minutes to index the collection on my machine.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{galago-index.png}
\caption{Indexing small Wikipedia collection using Galago.}
\label{gi}
\end{figure}

\subsubsection{Identifying 1,000 Words}

Once, indexing was done, I used ``dump-index'' command to see the indexed keywords in alphabetical order.

I realized that initial 14,135 records were starting with a numeric character. Those numbers were not good to illustrate the stemming. Hence, I decided to strip them off and collected next 1,000 words from the unstemmed index. Galago creates one stemmed and one unstemmed index together. For this purpose I used unstemmed index and in the next step, I did the stemming of the subset myself using some custom Ruby code.

List of all 1,000 words identified from the index is available in ``1000-plain-words.txt'' file in the submitted assignment archive.

\subsubsection{Stemming}

Using the files containing 1,000 candidate words from the previous step, I wrote a small Ruby script to create stem classes. This script uses Potter2 Stemmer library to create stems. The output was limited to the stem classes that have more than just single word. There were 104 such stem classes.

\begin{verbatim}
--------------------------------------------------------------
#!/usr/bin/ruby
# encoding: UTF-8

require "porter2stemmer"

@data = {}

File.open("1000-plain-words.txt").each_line{ |l|
  word = l.chop
  @data[word.stem] ||= []
  @data[word.stem] << word
}

@data.each do |k, v|
  if v.length > 1
    puts "/#{k} #{v.join(" ")}"
  end
end
--------------------------------------------------------------
\end{verbatim}

The above code outputs as ``/stem word1 word2 ...''. Sample output from the above code is as follows:

\begin{verbatim}
--------------------------------------------------------------
/abus abuse abused abusers abuses abusing abusive
/abut abuts abutting
/abysm abysm abysmal
/abyss abyss abyssal
/abyssinian abyssinian abyssinians
/academ academe academic academical academically academicals
        academics
/academician academician academicians
/academi academies academy academys
/acc acc acce acces
... ... ...
--------------------------------------------------------------
\end{verbatim}

Complete 104 line output can be found in ``stemmed-words.txt'' file.

\subsubsection{Association Measures}

Some custom Ruby code was written to calculate Dice's coefficient between all pairs of the stem classes.

$$ s = \frac{2|X \cap Y|}{|X| + |Y|} $$

This process was done in various steps. In the first step, following bit of code along with the stemmer script generated a batch query file for Galago.

\begin{verbatim}
--------------------------------------------------------------
@data.each do |k, v|
  if v.length > 1
    # Query for individual words
    v.each do |q|
      puts "<query>"
      puts "<number>#{k}-#{q}</number>"
      puts "<text>#combine(#feature:dirichlet(
            #extents:#{q}:part=postings()
            ))</text>"
      puts "</query>"
    end
    # Query for combination of two words
    v.combination(2).each do |c|
      puts "<query>"
      puts "<number>#{k}-#{c[0]}-#{c[1]}</number>"
      puts "<text>#combine(#feature:dirichlet(
            #extents:#{c[0]}:part=postings()
            )#feature:dirichlet(
            #extents:#{c[1]}:part=postings()
            ))</text>"
            puts "</query>"
    end
  end
end
--------------------------------------------------------------
\end{verbatim}

Sample output from the above code is as follows:

\begin{verbatim}
--------------------------------------------------------------
<query>
<number>aaa-aaa</number>
<text>#combine(#feature:dirichlet(
      #extents:aaa:part=postings()))
</text>
</query>
<query>
<number>aaa-aaas</number>
<text>#combine(#feature:dirichlet(
      #extents:aaas:part=postings()))
</text>
</query>
<query>
<number>aaa-aaa-aaas</number>
<text>#combine(#feature:dirichlet(
      #extents:aaa:part=postings())
      #feature:dirichlet(#extents:aaas:part=postings()))
</text>
</query>
... ... ...
--------------------------------------------------------------
\end{verbatim}

Complete query file with all possible single words and combinations of two words in the same stem cluster is available in ``batchquery.xml'' file.

The identifier key (``number'' tag) for each query was populated in the following way.

\emph{``stem-word1''} - for single words queries.

\emph{``stem-word1-word2''} - for combination of two words.

Once, this query file was generated, ``batch-search'' command of Galago was used to perform batch search. The output of the batch query was in TREC format. Original complete output can be found in ``'batchresult.trec' file.

This output file was then parsed using custom Ruby code to calculate the association measures.

Sample output of the TREC file looks like the following.

\begin{verbatim}
--------------------------------------------------------------
aaa-aaa Q0 Antonio_Pea_bdf0 1 -6.50511646 galago
aaa-aaa Q0 Omaha_Cardinals_ed7d 2 -6.55643463 galago
aaa-aaa Q0 Dark_Angel_(wrestler)_277d 3 -6.90676594 galago
aaa-aaa Q0 Notepad 4 -7.34984779 galago
aaa-aaa Q0 Funnel-web_spider 5 -7.41741562 galago
aaa-aaa Q0 Tom_Buckner_637d 6 -7.46555471 galago
aaa-aaa Q0 William_Ivins,_Jr._b8a8 7 -7.48517990 galago
aaa-aaa Q0 Geoff_Parsons_3535 8 -7.49786949 galago
... ... ...
--------------------------------------------------------------
\end{verbatim}

The ruby script to evaluate this TREC file is as follows.

\begin{verbatim}
--------------------------------------------------------------
#!/usr/bin/ruby
# encoding: UTF-8

@data = {}

File.open('batchresult.trec').each_line do |l|
  word = l.split(' ').first
  @data[word] ||= 0
  @data[word] += 1
end

@data.each do |k, v|
  w = k.split('-')
  if w.length == 3
    a = @data["#{w[0]}-#{w[1]}"].to_f
    b = @data["#{w[0]}-#{w[2]}"].to_f
    ab = a + b - @data["#{w[0]}-#{w[1]}-#{w[2]}"].to_f
    dc = (2 * ab) / (a + b)
    puts "#{k} => #{dc}" unless dc.zero?
  end
end
--------------------------------------------------------------
\end{verbatim}

Combination query in the batch file was an OR boolean query. Hence, I transformed the result using Set theory to get the results for AND query.

$$ |X \cap Y| = |X| + |Y| - |X \cup Y| $$

Output of this ruby script can be found in ``wordclusters.txt'' file. Sample output looks like this.

\begin{verbatim}
--------------------------------------------------------------
aaa-aaa-aaas => 0.07692307692307693
aac-aac-aace => 0.0
aafc-aafc-aafce => 0.0
aag-aag-aage => 0.0
aaliyah-aaliyah-aaliyahs => 1.0
aam-aam-aams => 0.0
aar-aar-aare => 0.0
ab-ab-abs => 0.0
aba-aba-abas => 0.4
abad-abad-abadal => 0.0
abandon-abandon-abandoned => 0.0967741935483871
abandon-abandon-abandoning => 0.0
abandon-abandon-abandonment => 0.07142857142857142
abandon-abandoned-abandoning => 0.05309734513274336
abandon-abandoned-abandonment => 0.05263157894736842
abandon-abandoning-abandonment => 0.0
... ... ...
--------------------------------------------------------------
\end{verbatim}

Here, the key format is same as used in the batch query file.

``aba-aba-abas = 0.4'' means, words aba and abas of stem class aba have association measure 0.4.


\subsubsection{Stem Clusters}

I have decided to choose a very small threshold because the collection itself was very small. Hence, probability of occurring all or most of the words of the same stem class in the same document was very little. Hence I removed only instances with association measure 0.0.

A filtered output is available in ``filteredwordclusters.txt'' file. It looks exactly the same as ``wordclusters.txt'' file except that it has no entries with 0.0 association measure.

\begin{verbatim}
--------------------------------------------------------------
aaa-aaa-aaas => 0.07692307692307693
aaliyah-aaliyah-aaliyahs => 1.0
aba-aba-abas => 0.4
abandon-abandon-abandoned => 0.0967741935483871
abandon-abandon-abandonment => 0.07142857142857142
abandon-abandoned-abandoning => 0.05309734513274336
abandon-abandoned-abandonment => 0.05263157894736842
... ... ...
--------------------------------------------------------------
\end{verbatim}

Looking at this output, several stem clusters were shorten in size. For example earlier, stem class ``accomplish'' has following words.

\emph{accomplish accomplished accomplishes accomplishment accomplishments}

It lost two valid words and became:

\emph{accomplished accomplishment accomplishments}

While, following class remain the same.

\emph{accord accordance accorded accordence according accordingly accords}

There were several examples showing valid words losing their cluster.


\subsection{Discussion}

Due to small size of the collection, most of the time, even the closely related words do not cluster, no matter how small the threshold is. Also, it was hard to find an example where wrongly stemmed words were separated. And if there were some (like ``abdul'' and ``abdull''), we could not make an argument about them because of the small size. It could have been separated due to probabilistic error.


\pagebreak


\section{Problem 6.3}

Implement a simple pseudo-relevance feedback algorithm for Galago search engine. Provide examples of query expansions that your algorithm does, and summarize the problems and success of your approach.


\subsection{Solution}

\begin{verbatim}
--------------------------------------------------------------
procedure pseudo_relevance_feedback(query)
  results = response_from_search_engine(query)
  top_ranked_pages = results.slice(0, N = 10)
  temp_text = top_ranked_pages.concat([titles and] snippets)
  temp_text = temp_text.clean(stop-words, tags and symbols)
  wordlist = temp_text.parse(count term frequencies)
  expanded_query = query + wordlist.top_keywords(!query)
  return response_from_search_engine(expanded_query)
end
--------------------------------------------------------------
\end{verbatim}

I started the local instance of Galago search engine and performed the following (semi-manual) operations in it to evaluate the success or failure of above algorithm.

I Searched for the query term \emph{``gandhi nehru''} in Galago as well as in Google and scraped top ten result snippets and titles. After cleaning and processing according to the algorithm above, I found the following top keywords.

\begin{verbatim}
--------------------------------------------------------------
Galago:

nehru(18)
gandhi(16)
jawaharlal(9)
indira(8)
family(6)
... ... ...
--------------------------------------------------------------
\end{verbatim}

\begin{verbatim}
--------------------------------------------------------------
Google:

nehru(12)
gandhi(11)
indira(6)
india(5)
jawaharlal(4)
family(4)
... ... ...
--------------------------------------------------------------
\end{verbatim}

Full results are available in ``pseudo-rel-galago.txt'' and ``pseudo-rel-google.txt'' files.

Gandhi and Nehru were two prime leaders of Indian independence. ``Jawaharal'' was the first name of Nehru and Indira was the daughter of Jawaharlal Nehru. Indira was married to the son of Mahatma Gandhi. This way, Nehru and Gandhi families were related. Having all these details in mind, above query expansion makes lots of sense.

Although, Wikipedia collection was small but Galago did equally well as Google did. Although, in some other queries, it failed because, there were not enough relevant results in Galago.


\subsection{Discussion}

From the above exercise, we may conclude that larger collection will improve the accuracy and success of the algorithm.

Inclusion of the titles along with the snippet text increased some frequencies of top keywords but overall order was not affected in the above example.

\pagebreak

\section{Problem 6.4}

Assuming you had a gazetteer of place names available, sketch out an algorithm for detecting place names or locations in queries. Show examples of the types of queries where your algorithm would succeed and where it would fail.

\subsection{Solution}

A simplest model could be to run all query terms through the index of place names or locations. But, the gazetteer could be huge in that case several lookups for every query will be costly process and will affect the response time. To solve this issue, we need to have a way to detect the possibility of location related terms in the query in advance.

A heuristic model can solve this problem to some extent by creating templates and passing the query through those templates. If it passes through the template then specific terms from the query phrase should be looked up in the gazetteer. Some example templates might look like the following.

\emph{``X, Y...''} are the place holders for possible place names.

\begin{verbatim}
--------------------------------------------------------------
direction from X to Y
distance between X and Y
flights from X to Y
flights to X
flights from X
weather in X
time in X
restaurants in X
shopping malls in X
historical places in X
elections in X
X park|st|av|rd|lake|beach...
... ... ...
--------------------------------------------------------------
\end{verbatim}

Using this technique will require several templates that will lead to greater percentage of false positives. Also, pure location name queries will not be detected using templates (unless we have a template as ``X'' that will produce huge number of false positives). One way to deal with this problem is to create a cache of frequently searched places using historical data from query log. And use that subset of places to lookup for every query term before running through the templates. If a historical query log is not available then feedback learning can be included in the algorithm itself. This way, look up cache can be built over the time from places detected with the help of template itself.

One thing to be considered is that, place names are not always unigrams. Also, these might contain stop-words like ``New York'', ``New Delhi''.

A look up cache can be a list of all place names (in the gazetteer) and search frequency (sorted by the search frequency). A threshold can be defined to manage the trade-off between cache size and additional time for each query.

Following is an algorithm with feedback learning embedded in it. That can help building frequently searched locations over the time.

\begin{verbatim}
--------------------------------------------------------------
procedure detect_location(query)
  read(query terms)
  if lookup_cached_places(query, frequency_threshold)
    return successful place detection
  else
    if pass_through_templates(query) == [X,Y...]
      if lookup_cached_places([X,Y...], frequency_threshold)
        add_or_increment_search_frequency([X,Y...])
        return successful place(s) detection
      else
        return unsuccessful place detection
      end
    end
    return place(s) not detected
  end
end
--------------------------------------------------------------
\end{verbatim}

\begin{verbatim}
--------------------------------------------------------------
procedure lookup_cached_places(query, frequency_threshold)
  if query exists in lookup_cache before frequency_threshold
    return TRUE
  end
  delayed_increment_search_frequency_if_exists_in_cache(query)
  return FALSE
end
--------------------------------------------------------------
\end{verbatim}

\begin{verbatim}
--------------------------------------------------------------
procedure pass_through_templates(query)
  foreach T in templates
    if query matches T
      return [X,Y...]
    end
  end
  return FALSE
end
--------------------------------------------------------------
\end{verbatim}


\subsection{Discussion}

The algorithm above can work with or without historical query log. Because, it has feedback learning embedded in it. For performance reasons, feedback can be stored separately and fed back into the system periodically in batches.

The failure and success of the above algorithm is a subjective issue. Since, it has machine learning capabilities embedded in the algorithm, hence, the success rate may improve over over the time. Also, templates are playing major role in place detection, but increased number of templates may cause several false positives. But, false positives will not pass through the entire procedure because in the next step they will be evaluated against the partially cached (frequently searched places of) gazetteer.

\emph{``elections in Norfolk''} - success

\emph{``elections in 2013''} - fail

Above example query set illustrates where template may detect correct or false place name. That will then be stored for feedback learning. At the time of learning, the algorithm makes sure that increment to the search counter is only made to the terms that exist in the gazetteer.

False negatives can be handled in two ways, by adding more templates in the template set and/or by relaxing the search frequency threshold (i.e. having more records in the cache to process).


\pagebreak

\section{Problem 6.5}

Describe the snippet generation algorithm in Galago. Would this algorithm work well for pages with little text content? Describe in detail how you would modify the algorithm to improve it.

\subsection{Solution}

Snippet generator implementation in Galago is very simple. It works on simple keyword matching in various regions of the document then combining those regions up to threshold of max snippet size. It simply ignores the later portions of the document where matches were found if the threshold reaches earlier (irrespective of the importance of various segments).

I have learned following procedure used in the code for snippet generation in Galago. \footnote[1]{http://www.galagosearch.org/galagosearch-core/xref/org/galagosearch/core/store/\\SnippetGenerator.html}

\begin{verbatim}
--------------------------------------------------------------
procedure generate_snippet(doc)
  regions = lookup_matching_keywords_and_mark_regions(doc)
  highlight(matching_terms)
  regions = merge_overlapped_regions(regions)
  snippet = concat(regions)
  return snippet.slice(0, max_snippet_size)
end
--------------------------------------------------------------
\end{verbatim}

Above procedure can be described in plain English, as follows.

Create an array of regions where matches were found.\\*
Highlight matching terms.\\*
Compare those regions to check for full or partial overlap.\\*
Merge overlapped regions.\\*
Combines various final regions from beginning to end until snippet size threshold is reached.

Snippet generator in the Galago is very basic because advanced snippet generators may generate snippets even if there is no exact query term matches were found but the document was relevant. While Galago shows snippets if there is an "exact" match of query terms exists in the document. Although, search algorithm makes use of stemmed index but the snippet generator does not.

Figure \ref{ghut} and Figure \ref{ghuts} will illustrate my point. When I search for the term ``hut'' it gave me a set of results, all were populated with some snippet text except the one highlighted above. Now, if the search query is plural form, ``huts'', result set is same but this time the one that is highlighted above gets some snippet text. But, in contrast, those highlighted bellow lose the snippets. Although, the very first result has some snippet in both the cases as it has both singular and plural forms of the term in the document.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{galago-snippet-hut.png}
\caption{Search result for the term ``hut''}
\label{ghut}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{galago-snippet-huts.png}
\caption{Search result for the term ``huts''}
\label{ghuts}
\end{figure}


\subsubsection{Small Document Snippets}

It seems that it does not matter if the document has little text content or big, if some part of it exactly matches the query term, Galago will show a snippet. Since, stemmed index is not being used while generating snippets. Hence, there is a fair chance that document with small text content might not contain the exact form (but other forms of the same stem class may be there) of the word as in the query string.

One possible solution to this problem could be inclusion of stemming logic in the snippet generation algorithm. But if the text content is very small (smaller than max snippet size threshold) like a tweet, then we can just show the whole text instead of passing it through complex algorithm to generate snippet. If the document size is twice or thrice as big as the max snippet size threshold and no exact match was found to generate a snippet then we can pick the middle region of the document as snippet. Something is better than nothing.

\subsection{Discussion}

I have no documented proof but, I would further claim that if there is a document with search term appearing in title of the HTML document but not in the main body, Galago will fail to produce a snippet for that as well.

There seems to be some code to discover the most suitable region based on some ranking techniques in the Galago code base. But, that segment is not used in the actual code flow yet. The comment says that it has some serious performance issues and needs optimization.


\pagebreak


\section{Problem 6.7}

Implement a simple algorithm that selects phrases from the top-ranked pages as the basis for the result clusters. Phrases should be considered as any two-word sequence. Your algorithm should take into account phrases frequency in the results, phrase frequency in the collection, and overlap in the clusters associated with the phrases.

\subsection{Solution}

I have created a simple vector space model using two-word sequence phrases as vectors. The value or weight for each vector is the phrase frequency in the document multiplied by inverse document phrase frequency. It is derived from the idea of TF*IDF. A matrix holds these values as phrases in column and documents in rows.

\begin{verbatim}
--------------------------------------------------------------
# Matrix storing phrase-frequency * inverse-document-frequency
document_phrase_vector = [documents * phrases]

procedure search(query)
  results = response_from_search_engine(query)
  top_ranked_pages = results.slice(0, N = 10)
  result_phrases = phrase_extractor(top_ranked_pages)
  results_vector = document_phrase_vector.filter_rows(results)
  results_vector = results_vector.filter_cols(result_phrases)
  K = top_result_phrase_count(results_vector)
  clustered_results = k_means_cluster(results, K, overlap = F)
  display(clustered_results)
end
--------------------------------------------------------------
\end{verbatim}

A sub-routine that helps extracting pairs of adjacent words and phrases in the top-ranked pages (or any set of pages that are passed as parameter).

It is assumed that we already have some routine to clean the html pages and transform them in a plain text file with space separated words without any HTML tags, new-lines, punctuation marks or other symbols.

\begin{verbatim}
--------------------------------------------------------------
procedure phrase_extractor(top_ranked_pages)
  page = top_ranked_pages.concat
  page = page.clean // make simple space separated text file of words
  words = page.split(' ')
  temp = []
  i = 1
  while(i < words.length)
    temp.push(words[i-1] + ' ' + words[i])
  end
  return temp
end
--------------------------------------------------------------
\end{verbatim}

This is a sub-routine that discovers top-ranked phrases in the result set based on some threshold. It then returns the number of such phrases that can be used to estimate the number of clusters in the clustering algorithm.

\begin{verbatim}
--------------------------------------------------------------
procedure top_result_phrase_count(results_vector)
  phrase_weights = column_sum(results_vector)
  phrase_weights = phrase_weights.sort(DESC)
  phrase_weights = phrase_weights.filter(value > threshold)
  return phrase_weights.count
end
--------------------------------------------------------------
\end{verbatim}

\subsubsection{Algorithm Explanation}

When a query is being fired, results are extracted from underlying search engine. Assuming that results are sorted based on some ranking parameters by the underlying search engine itself. Top N (for example 10) pages from the result set are used for clustering the whole result set. A ``phrase\textunderscore extractor'' routine extracts all two-word phrases from the N top ranked result pages. A sub-matrix is then created from global document phrase vector matrix by filtering only result pages and phrases in the top ranked result pages. Clustering is then performed on this sub matrix using K-means clustering algorithm without overlaps in the clusters. Number of clusters is not the same as distinct phrases in the result set. That number is identified in another procedure called ``top\textunderscore result\textunderscore phrase\textunderscore count''.

The ``top\textunderscore result\textunderscore phrase\textunderscore count'' procedure is simple. It sums up the values of each phrase in the results vector (column-wise sum). This sum gives an idea about the aggregated importance of certain phrases in the result set. Phrases are then sorted in descending order of aggregated values and counted up to a minimum threshold on values.


\subsection{Discussion}

One question arises, whether the top phrases (top K phrases identified in the ``top\textunderscore result\textunderscore phrase\textunderscore count'' procedure) will fall in distinct clusters? At this point, I have no proof that the final clusters in the result set will have all those top-ranked phrases in distinct clusters. Intuitively, it is less likely to happen, as those top ranked phrases might cluster together. But, this is something to be explored and examined further. But, ``top\textunderscore result\textunderscore phrase\textunderscore count'' procedure gives a good measure to estimate, how many clusters should be there (dynamically depending on the result set) in the final result?


\pagebreak


\section{Problem 7.2}

Can you think of another measure of similarity that could be used in the vector space model? Compare your measure with the cosine correlation using some example documents and queries with made-up weights. Browse the IR literature on the Web and see whether your measure has been studied (start with van Rijsbergen's book).

\subsection{Solution}

Another similarity measure that I can think of is ratio between the mod of difference of the two vectors and mod of their sum. It is actually the distance measure. Hence, more the distance, less is the similarity.

$$ D_{d, q} = \frac{|\vec{d} - \vec{q}|}{|\vec{d} + \vec{q}|} $$

Where \emph{d} and \emph{q} are the two vectors in the vector space (typically document and query vectors but, these can be any two documents as well.)

These vectors usually have positive coefficients in case of document vectors. But, negative queries can have negative coefficients as well. In normal cases this measure can work well but it may fail or stretch the measure in some cases.

\subsubsection{Example}

Consider a small document ``D'' and a query ``Q'' as follows.

\begin{verbatim}
--------------------------------------------------------------
    D = (book pen book ink book)
    Q = (book pen)
--------------------------------------------------------------
\end{verbatim}

Now I will compute the cosine correlation as well as my distance measure for this document and query to illustrate the working. For the sake of simplicity, I am not putting the vector bar in the formulas.

$$ d = 3*book + 1*pen + 1*ink $$
$$ q = 1*book + 1*pen $$

In this example, book, pen and ink are the only words in the vector space. These words are representing independent unit vectors (assuming that these words are independent in the language model) making a right angle between every couple of vectors.

By doing simple vector subtraction and addition, we get the following vectors.

$$ d - q = 2*book + 0*pen + 1*ink $$
$$ d + q = 4*book + 2*pen + 1*ink $$

Coefficients of the two vectors, their difference and their sum can be calculated easily using standard vector equation.

$$ |d| = \sqrt{(3^2 + 1^2 + 1^2)} = 3.3167 $$
$$ |q| = \sqrt{(1^2 + 1^2)} = 1.4142 $$

$$ |d - q| = \sqrt{(2^2 + 0^2 + 1^2)} = 2.2361 $$
$$ |d + q| = \sqrt{(4^2 + 2^2 + 1^2)} = 4.5826 $$

Now that we have intermediate steps and their numeric values, we can proceed to calculate the measures.

$$ Cosine\,similarity = \frac{(1*3) + (1*1)}{(3.3167*1.4142)} = 0.8528 $$
$$ My\,distance\,measure = \frac{2.2361}{4.5826} = 0.4879 $$

I can repeat this process for other vectors to illustrate more examples. But, I believe, It is a very simple distance measure and one example is sufficient to illustrate the working.


\subsection{Discussion}

To study the behaviour of this distance measure, consider one dimensional vector (for the sake of simplicity) for the following cases.

\subsubsection{CASE 1: Short and Large Coefficients}

On one dimensional vector space, suppose coefficients of \emph{d} and \emph{q} are 2 and 1 respectively. The distance measure will produce a value of 1/3. Now, change the coefficients to 100 and 99. In the space, the two are just one unit away as in previous case but the distance measure will be 1/199. Although, it has stretched/squeezed the linear distance, intuitively, this is the desired result. Because, large coefficients mean that there are several occurrences of the property represented by that dimension. This means, two documents having a term 100 times and 99 times respectively are more similar (less distant) than the two documents having a term twice and once respectively.

\subsubsection{CASE 2: Coefficients with Opposite Signs}

If the coefficient of \emph{d} is positive, the coefficient of \emph{q} is negative and its modulus is less than that of \emph{d}, it will work fine. But, when the two coefficients are equal and opposite, the distance measure will be infinite (extremely non-relevant). Beyond that point, distance measure will start giving continuously decreasing finite values that is not valid (and counter-intuitive). This behaviour can be observed in Figure \ref{distmm}.

Later, I was exploring the Web to see if this distance measure was studied earlier or not, I found that an improved version (namely \emph{``Bray-Curtis''}) of this measure has already been studied.\cite{bray1957ordination}\cite{celebi2010alternative}

They have an enhancement in the distance measure by not taking the mod of denominator (d + q). This eliminates the problem due to opposite coefficients described above. Because, it results in negative distance measures after critical point that can be safely considered as non-relevant. This behaviour can be observed in Figure \ref{distbc}.


\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{distmm.png}
\caption{My distance measure for d=2}
\label{distmm}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{distbc.png}
\caption{Bray-Curtis distance measure for d=2}
\label{distbc}
\end{figure}


Rscript used to generate Figure \ref{distmm} and Figure \ref{distbc} plots is follows.

\begin{verbatim}
--------------------------------------------------------------
fn <- function(x) abs(2-x)/abs(2+x)
fn1 <- function(x) abs(2-x)/(2+x)

png("distmm.png", width=500, height=500)
plot(fn, -10, 10, type="l", xlab="q (Query Vector)",
     ylab="Distance Measure", xaxs="i", yaxs="i",
     ylim=c(-10, 10))
lines(c(-10, 10), c(0, 0), lty=3)
lines(c(0, 0), c(-10, 10), lty=3)
dev.off()

png("distbc.png", width=500, height=500)
plot(fn1, -10, 10, type="l", xlab="q (Query Vector)",
     ylab="Distance Measure", xaxs="i", yaxs="i",
     ylim=c(-10, 10))
lines(c(-10, 10), c(0, 0), lty=3)
lines(c(0, 0), c(-10, 10), lty=3)
dev.off()
--------------------------------------------------------------
\end{verbatim}


\pagebreak



\section{Problem 7.5}

Implement a BM25 module for Galago. Show that it works and document it.

\subsection{Solution}

I tried writing the BM25 module in Java. But, it was not a successful effort (till the point of compilation and execution). Here is my code.

\begin{verbatim}
--------------------------------------------------------------
/*
 * This is a simple BM25 score calculator that is not
 * in running condition yet.
 * It has some parameters pre-defined and hard-coded in it.
 */
public class BM25 {
  /* Constructor method to initiate the parameters */
  public BM25() {
    b = 0.75;
    k = 1.2;
    cl = 10000.0; // Collection length
    dc = 100.0; // Document count
    df = 50.0; // Document frequency
    // IDF calculation
    idf = Math.log((dc - df + 0.5) / (df + 0.5));
  }

  /* Publically exposed method to calculate the score */
  public double calculate(int c, int l) {
    // Actual BM25 formula
    result = idf * (c * (k + 1))
                  / (c + (k * (1 - b + (b * l / (cl / dc)))));
    return result;
  }
}
--------------------------------------------------------------
\end{verbatim}


\subsubsection{Code Available in Galago Source}

The code for BM25 is available in the latest version of the Galago code repository. But, it is not compiled and not included in the JARs of binary package. Hence, when I tried to run a test query against ``BM25'' scoring module, it was giving me errors.

Here is how the code looks like.

\begin{verbatim}
--------------------------------------------------------------
@RequiredStatistics(statistics = {"collectionLength",
                                  "documentCount"})
public class BM25Scorer implements ScoringFunction {
  double b;
  double k;
  double avgDocLength;
  double idf;

  public BM25Scorer(Parameters parameters,
                    CountValueIterator iterator)
                    throws IOException {
    b = parameters.get("b", 0.75D);
    k = parameters.get("k", 1.2D);

    long collectionLength = parameters.get("collectionLength",
                                           0L);
    long documentCount = parameters.get("documentCount", 0L);
    avgDocLength = (collectionLength + 0.0) / (documentCount
                                               + 0.0);

    long df = 0;
    if (parameters.containsKey("df")) {
      df = parameters.get("df", 0L);
    } else {
      df = iterator.totalEntries();
    }
    idf = Math.log((documentCount - df + 0.5) / (df + 0.5));
  }

  public double score(int count, int length) {
    double numerator = count * (k + 1);
    double denominator = count + (k * (1 - b + (b
                                  * length / avgDocLength)));
    return idf * numerator / denominator;
  }

  public String getParameterString(){
    return "bm25.b=" + b + ",bm25.k="+k;
  }
}
--------------------------------------------------------------
\end{verbatim}

A test query against this module.

\begin{verbatim}
--------------------------------------------------------------
<parameters>
  <query>
    <number>book</number>
    <text>#combine(#feature:bm25(
                   #extents:test:part=stemmedPostings()))
    </text>
</query>
</parameters>
--------------------------------------------------------------
\end{verbatim}

And the error I encountered.

\begin{verbatim}
--------------------------------------------------------------
Couldn't find a class for the feature named bm25.
--------------------------------------------------------------
\end{verbatim}

\subsection{Discussion}

I am not comfortable with Java coding and specially packaging. Also, I am not very familiar with the Galago code-base (although, I tried my best to understand it).Hence, it was hard for me to build a successful module in a very modular fashion, along with proper parameters and exception handling etc. Hence, I tried to assume that certain things will be available to me when I import some packages in my code. Then I did the coding mainly for the metamathematical portion of the algorithm. Later I discovered that there is an untested code in the code-base of Galago for BM25. I realized that the portion I did was almost correct as compared to the code available in the repository. Hence, I included my code along with the code available in the repository. I did some in-line documentation of my code as well. I did not make any further changes in my code after getting the code in the repository to avoid any honour code violation. For the sake of comparison, I have included both of them.

Probably, I would have not attempted this question but, there were not many choices left. This entire assignment was very challenging and long. I spend several days and couple off sleepless nights doing this assignment (in last two weeks) but, I am happy that I got some insight of Galago search engine (because there was no way to avoid that).

Often times I thought, I wont be able to do this assignment at all except couple of questions. But, I studied related material again and again and now I am satisfied with what I have done so far.


\pagebreak


\section{Problem 7.8}

Using the Galago implementation of query likelihood, study the impact of short queries and long queries on effectiveness. Do the parameter settings make a difference?

\subsection{Solution}

Galago uses \emph{``Dirichlet''} algorithm that is a derived form of pure \emph{``query likelihood''} algorithm. Biggest drawback of pure \emph{``query likelihood''} algorithm is its failure to handle query term(s) is the query phrase with zero relevance. Since, it uses multiplicative aggregation that makes aggregated similarity zero if a single term in the query phrase has zero similarity with the document (irrespective of the relevance of other terms in the query phrase).

\emph{``Dirichlet''} algorithm uses smoothing function to overcome the effect of multiplication by zero. This way, a small fraction of relevance (from relevant terms) is distributed among all the terms that did not appear in the document.

Galago has two classes \emph{``DirichletScorer''}\footnote[2]{http://www.galagosearch.org/galagosearch-core/xref/org/galagosearch/core/scoring/\\DirichletScorer.html} and \emph{``DirichletSmoother''}\footnote[3]{http://www.galagosearch.org/galagosearch-core/xref/org/galagosearch/core/scoring/\\DirichletSmoother.html} that work in conjunction to perform the operations.

Here is the code snippet from ``DirichletScorer.java'' file of Galago that shows the fact that it is using \emph{query likelihood} formula.

\begin{verbatim}
--------------------------------------------------------------
public double score(int count, int length) {
  double numerator = count + (mu * background);
  double denominator = length + mu;
  return Math.log(numerator / denominator);
}
--------------------------------------------------------------
\end{verbatim}

Here is the code snippet from ``DirichletSmoother.java'' file of Galago that shows the formula used to smoothing the term and document relevance measure.

\begin{verbatim}
--------------------------------------------------------------
public double score(int count, int length) {
  double numerator = count + (mu * background);
  double denominator = length + mu;
  return Math.log(numerator / denominator);
}
--------------------------------------------------------------
\end{verbatim}

\subsubsection{Short and Long Queries}

Average length queries are best performing queries because they gain enough topic knowledge encoded in them. While, short queries may be ambiguous and grab lots of noise in the result set. At the same time, very long queries may be penalized due to several non-matching terms. Although, smoothing operation normalize the effect of multiplication by zero to some extent but it does not protect overall relevance drop. For example if there is a query phrase with two highly related terms (against certain documents) but, it has five non-related terms as well. Smoothing will give very small fraction of relevance to those non-related terms but overall relevance will drop significantly. Because of the fact that multi plication of two big numbers with five very small numbers will effectively be very small number.

Let us consider a practical example. Suppose there is a single word query ``apple'', search result might be ambiguous because apple is being used in various different contexts. Now, we make it ``apple computer'' or ``apple computer screen'', it becomes more specific and it now has topic semantics encoded. because it is not going to be confused by apple fruit. Hence, it is expected to perform better in terms of relevance. Adding some more keywords might make the query so specific that a relevant document (with all or most of the query terms) might be hard to find. That will affect the recall a lot if the focus is on maintaining good precision.

\subsection{Discussion}

Parameter setting will have minimal effect on average size queries. But, it may play significant role in adjusting the trade-off between precision and recall for (specially for long queries with several non-matching terms). It usually does not affect the ordering of top-ranked results if the parameters are not altered significantly.



\bibliographystyle{plain}
\bibliography{ref}
\end{document}